{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Heisig_Task.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk69gV1rNK46"
      },
      "source": [
        "# **Challenge**: Write a python script that downloads all Technical Drawings from https://www.heisig.com/aktuelle-auftraege/ and stores them locally.\r\n",
        "\r\n",
        "#Project Details:\r\n",
        "1. Allow user to  save all drawings\r\n",
        "2. Allow user to scrape all titles and detailes for every drawing\r\n",
        "3. save all data and export in Json and CSV files\r\n",
        "\r\n",
        "#Future works:\r\n",
        "1. Allow user to choose how many drawings to download\r\n",
        "2. compute consuming time for excuation \r\n",
        "3. export all file names with consume time in file\r\n",
        "\r\n",
        "\r\n",
        "Have enjoy scraping :) \\\r\n",
        "ALhasan Alshaebi\\\r\n",
        "aatl2012@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pUvnCVbNHdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636568e8-ad38-48b0-9f72-7d048c2a45ee"
      },
      "source": [
        "from bs4 import BeautifulSoup, SoupStrainer\r\n",
        "import requests\r\n",
        "from urllib.parse import urlsplit\r\n",
        "import urllib.request\r\n",
        "import requests.exceptions\r\n",
        "import regex as re\r\n",
        "import csv\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import json\r\n",
        "#import shutil\r\n",
        "!pip install wget\r\n",
        "import wget\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=009808193cb26a90e5a7cbb6b1d6c70053ecacfc3a64ebb437db47cb1dcc329c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9wOSvFFyVgS"
      },
      "source": [
        "####################################################\r\n",
        "# set header to avoid connection server error \r\n",
        "headers={'Accept-Encoding': 'gzip, deflate, sdch',\r\n",
        "    'Upgrade-Insecure-Requests': '1',\r\n",
        "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\r\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\r\n",
        "    'Cache-Control': 'max-age=0',\r\n",
        "    'Connection': 'keep-alive',}\r\n",
        "# Crawler class to starting crawler\r\n",
        "class Crawler:\r\n",
        "    def __init__(self, titleTag,bodyTag):\r\n",
        "        #self.url = url\r\n",
        "        self.titleTag = titleTag\r\n",
        "        self.bodyTag = bodyTag\r\n",
        "        ########################################\r\n",
        "    def getPage(self, url):\r\n",
        "        try:\r\n",
        "            req = requests.get(url,headers=headers,allow_redirects=False)\r\n",
        "        except requests.exceptions.RequestException:\r\n",
        "            return None        \r\n",
        "        return BeautifulSoup(req.text, 'lxml') # return html from website\r\n",
        "    ####################################################\r\n",
        "    def parse(self, url):\r\n",
        "        \"\"\"\r\n",
        "        Extract content from a given page URL\r\n",
        "        \"\"\"\r\n",
        "        soup = self.getPage(url)\r\n",
        "        \r\n",
        "        drw=soup.find(\"div\",class_=\"col-md-9\")\r\n",
        "        #print(drw)\r\n",
        "        quotes=[]\r\n",
        "        urls=[]\r\n",
        "        # save all drawings links\r\n",
        "        for title in drw.find_all(\"div\",class_=self.titleTag):\r\n",
        "          tit={}\r\n",
        "          tit=title.get_text()\r\n",
        "          urls.append(tit)\r\n",
        "        print(\"Number of Drawings: \",len(urls))\r\n",
        "        # extract all related information to drawings\r\n",
        "        for ind,i in enumerate(drw.find_all(\"div\",class_=self.bodyTag)):\r\n",
        "          quote = {} \r\n",
        "          quote[\"Title\"]=urls[ind] # get title of drawing\r\n",
        "          quote[\"Content\"]=i.get_text() # get details of drawing\r\n",
        "            #print(i)\r\n",
        "          links = \"\"\r\n",
        "          #print(\"Donsse\")\r\n",
        "          c=0\r\n",
        "          # extract all Links\r\n",
        "          for link in i.findAll('a', attrs={'href': re.compile(\"^/Zeichnung\")}):\r\n",
        "            quote_link = {}\r\n",
        "            c=c+1\r\n",
        "            #quote_link[\"link\"]=link.get('href')\r\n",
        "            lnk=\"https://www.heisig.com/\"+link.get('href')\r\n",
        "            #s = urllib.request.urlopen(lnk).read().decode('utf-8')\r\n",
        "            #file_name, headerss = urllib.request.urlretrieve(lnk)\r\n",
        "            #with urllib.request.urlopen(url) as response, open(\"/content/images\", 'wb') as out_file:\r\n",
        "              #shutil.copyfileobj(response, out_file)\r\n",
        "            #urllib.request.urlretrieve(lnk,str(c)+\".tif\")\r\n",
        "\r\n",
        "            # save locally all drawings\r\n",
        "            wget.download(lnk)\r\n",
        "            links=links+lnk+\" , \"\r\n",
        "          quote[\"Links\"]=links\r\n",
        "          quotes.append(quote)\r\n",
        "          # Export all data in files \r\n",
        "        export_table_and_print(quotes)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AhWKYl0yKrA"
      },
      "source": [
        "# Here we export all extract data in files Json and CSV .\r\n",
        "def export_table_and_print(all_data):\r\n",
        "  # export name of file from url\r\n",
        "  Fname = \"{0.netloc}\".format(urlsplit(MainPage)).replace(\"www.\",\"\").replace(\".com\",\"\")\r\n",
        "  # make header for out data\r\n",
        "  table = pd.DataFrame(all_data, columns=['Title', 'Content', 'Links'])\r\n",
        "  table.index = table.index + 1\r\n",
        "  # Export Drawings  in json file\r\n",
        "  if not os.path.isfile(Fname+'.json'):\r\n",
        "    with open(Fname+'.json', 'w') as outfile:\r\n",
        "        json.dump(all_data, outfile,indent=4)\r\n",
        "    print(\"Done file created\")\r\n",
        "  else:\r\n",
        "    # If file already exist will be appended data\r\n",
        "    with open(Fname+'.json', 'a') as outfile:\r\n",
        "        json.dump(all_data, outfile,indent=4)\r\n",
        "    print(\"Done Data appended\") \r\n",
        "  # Export Drawings  in csv file   \r\n",
        "  with open(Fname+\".csv\", 'w', newline='') as f: \r\n",
        "    w = csv.DictWriter(f,['Title','Content','Links']) \r\n",
        "    w.writeheader() \r\n",
        "    for quote in all_data: \r\n",
        "        w.writerow(quote) \r\n",
        "####################################################\r\n",
        "####################################################"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z6MjUTNNrBS",
        "outputId": "2a9ade31-c411-432d-8561-04c04cb7c7be"
      },
      "source": [
        "#@markdown #Drawings Downloading:\r\n",
        "\r\n",
        "# map main page URL\r\n",
        "MainPage = 'https://www.heisig.com/aktuelle-auftraege' #@param {type:\"string\"}\r\n",
        "# to extract more than one order\r\n",
        "Aktuelle_auftraege = '/drehen,/fraesen-bohrwerksarbeiten' #@param {type:\"string\"}\r\n",
        "#@markdown Optional:\r\n",
        "# select all or a specific number of Drawings\r\n",
        "Num_Drawings = 764 #@param {type:\"slider\", min:0, max:10000, step:1}\r\n",
        "# set crawl all articles\r\n",
        "All_Drawings = True #@param {type:\"boolean\"}\r\n",
        "crawler = Crawler(\"AnfrageHL\",\"AnfrageIN\")\r\n",
        "for order in Aktuelle_auftraege.split(\",\"):\r\n",
        "  crawler.parse(MainPage+order)\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "Done Data appended\n",
            "13\n",
            "Done Data appended\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}